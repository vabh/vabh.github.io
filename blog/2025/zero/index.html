<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ZeRO stage-1 | Anuvabh Dutt </title> <meta name="author" content="Anuvabh Dutt"> <meta name="description" content="Mostly notes for myself "> <meta property="og:site_name" content="Anuvabh Dutt"> <meta property="og:type" content="article"> <meta property="og:title" content="Anuvabh Dutt | ZeRO stage-1"> <meta property="og:url" content="https://vabh.github.io/blog/2025/zero/"> <meta property="og:description" content="Mostly notes for myself "> <meta property="og:image" content="https://vabh.github.io/assets/img/posts/zero/sharded.svg"> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="ZeRO stage-1"> <meta name="twitter:description" content="Mostly notes for myself "> <meta name="twitter:image" content="https://vabh.github.io/assets/img/posts/zero/sharded.svg"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.css" integrity="sha256-uRX+PiRTR4ysKFRCykT8HLuRCub26LgXJZym3Yeom1c=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://vabh.github.io/blog/2025/zero/"> <script src="/assets/js/theme.js?9b8fbe7f3c8704b08b2486b84879fe61"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Anuvabh</span> Dutt </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">ZeRO stage-1</h1> <p class="post-meta"> Created on October 12, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ml"> <i class="fa-solid fa-hashtag fa-sm"></i> ml</a>   <a href="/blog/tag/user-guide"> <i class="fa-solid fa-hashtag fa-sm"></i> user_guide</a>   ·   <a href="/blog/category/pytorch"> <i class="fa-solid fa-tag fa-sm"></i> pytorch</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="exploring-zero">Exploring ZeRO</h3> <p>This post explores the use of <a href="https://docs.pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer" target="_blank" rel="external nofollow noopener">ZeRO optimizer</a> from PyTorch. This is useful when training with DDP and you are bottlenecked by GPU memory. ZeRO optimizer shards the optimizer states across devices, leading to reduced memory usage per device. The <a href="https://arxiv.org/pdf/1910.02054" rel="external nofollow noopener" target="_blank">ZeRO paper</a> goes into details. Here we explore ZeRO stage-1.</p> <p>There is really no reason not to shard the optimizer states. It’s relatively straightforward to use with the ZeRO API. Note that there are some limitations, which will go away when we migrate to FSDP. However, the point of this post is to show that adding a few lines of code can already be quite beneficial.</p> <p>Before, going into the code, let’s review what it means to <em>shard</em> the optimizer states and how much memory we can expect to save.</p> <p>I will use the small gpt config from <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="external nofollow noopener">nanoGPT</a>, which has \(124M\) params, as an example. If you load it up and do a training run, you will see that the memory usage is around \(12.3\) GiB.</p> <h3 id="memory-usage">Memory usage</h3> <p>Let’s break down this memory usage. A good reference is <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=memory_for_weights/grads/optimizer_states" rel="external nofollow noopener" target="_blank">here</a>. GPU memory comprises the following:</p> <ul> <li>M_p: The model parameters</li> <li>M_p32: A copy of the parameters in full precision, when using mixed precision training</li> <li>M_g: Gradients</li> <li>M_opt: Optimizer states (momentum in SGD, momentum and variance in Adam)</li> <li>M_act: Activations</li> <li>Other things like cuda kernels and temporary buffers</li> </ul> <p>The figure below depicts the memory usage:</p> <p><img src="/assets/img/posts/zero/memory_usage.svg" alt="" class="img-fluid d-block mx-auto"></p> <p>Apart from the activations, the memory for the components can be computed in an architecture agnostic manner. Let’s use our observations for the total memory usage of the \(124M\) model and get the memory used by activations (and the overhead things). This will let us estimate the memory usage after sharding the optimizer states.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total parameters: N = 124_000_000
GPU memory per device: M = 12.3 * 2 ** 30 bytes

M_p = 2 * N # It takes two bytes to store 1 parameter in float16 / bfloat16
M_g = 2 * N;
M_p32 = 4 * N; # in mixed precision training, we keep a copt of the weights in float32
M_opt = 8 * N

The memory for activations is:
M_act = M - M_p - M_g - M_p32 - M_opt
M_act = 10.49 * 2 ** 30 bytes
</code></pre></div></div> <p>If we have <code class="language-plaintext highlighter-rouge">d</code> GPUs, and we shard the optimizer states across them, the expected memory usage per GPU can be calculated as:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>M_zero1 = M_act + M_p + M_g + M_p32 + (M_opt / d)
M_zero  = 11.53
</code></pre></div></div> <p>The memory savings is entirely dependent on <code class="language-plaintext highlighter-rouge">d</code>. For the \(124M\) model, around 6% memory savings are to be expected.</p> <p>After sharding, the optimizer states are now split across the devices. In the following diagram, for 2 devices, the shaded regions show the optimizer states which are no longer replicated across the two GPUs.</p> <p><img src="/assets/img/posts/zero/sharded.svg" alt="" class="img-fluid d-block mx-auto"></p> <p>So how do we do this in code? PyTorch makes it really easy to instantiate the ZeRO version of an optimizer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.distributed.optim</span> <span class="kn">import</span> <span class="n">ZeroRedundancyOptimizer</span> <span class="k">as</span> <span class="n">ZeRO</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="nc">ZeRO</span><span class="p">(</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
    <span class="n">optimizer_class</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">,</span>
    <span class="n">parameters_as_bucket_view</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">overlap_with_ddp</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># we will get to this later
</span>    <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>The optimizer parameters are passed as keyword arguments to the <code class="language-plaintext highlighter-rouge">ZeRO</code> constructor. Here we pass the <code class="language-plaintext highlighter-rouge">lr</code> and <code class="language-plaintext highlighter-rouge">betas</code> for <code class="language-plaintext highlighter-rouge">AdamW</code>.</p> <p>Let us now instantiate models of varying sizes and check the memory usage with and without sharding the optimizer states. The sequence length is set to \(1024\) tokens and the batch size is \(12\).</p> <table> <thead> <tr> <th>Memory</th> <th>124M</th> <th>353M</th> <th>772M</th> </tr> </thead> <tbody> <tr> <td>without sharding</td> <td>12.34 GB</td> <td>23.20 GB</td> <td><span style="color:#d9534f;">OOM</span></td> </tr> <tr> <td>with sharding</td> <td>11.15 GB</td> <td>21.26 GB</td> <td>35.93 GB</td> </tr> </tbody> </table> <p>We see that the actual memory usage is what our calculation above predicted. Note that if we keep the batch size fixed, the \(772M\) model is ‘out-of-memory’ <em>without sharding</em>.</p> <p>So is this a free lunch? Since we split the optimizer state across different devices, the devices need to communicate and update the copy of their weights accordingly. So whenever an optimization step happens, the weights need to be synced.</p> <h3 id="communication-cost">Communication cost</h3> <p>Let’s expand the table from above and look at the time it takes for each optimization step, on 8 * A100s.</p> <table> <thead> <tr> <th>Metric</th> <th>ZeRO</th> <th>124M</th> <th>353M</th> <th>772M</th> </tr> </thead> <tbody> <tr> <td>Memory</td> <td>✗</td> <td>12.34 GB</td> <td>23.20 GB</td> <td>OOM</td> </tr> <tr> <td>Memory</td> <td>✓</td> <td>11.15 GB</td> <td>21.26 GB</td> <td>35.93 GB</td> </tr> <tr> <td>Time/step</td> <td>✗</td> <td>345.83 ms</td> <td>907.96 ms</td> <td>—</td> </tr> <tr> <td>Time/step</td> <td>✓</td> <td>367.77 ms</td> <td>962.39 ms</td> <td>1943.65 ms</td> </tr> <tr> <td>MFU</td> <td>✗</td> <td>48.61%</td> <td>52.96%</td> <td>—</td> </tr> <tr> <td>MFU</td> <td>✓</td> <td>46.05%</td> <td>49.95%</td> <td>52.68%</td> </tr> </tbody> </table> <p>We see that the way we have used ZeRO, it is slightly slower than the non-sharded version. However, the good news is that we can overlap the communication step with the DDP communication. This needs a bit of extra work. You have to add specific hooks and not call the optimizer step directly.<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">hook_with_zero_step</span><span class="p">,</span> 
    <span class="n">hook_with_zero_step_interleaved</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="n">torch.distributed.algorithms.ddp_comm_hooks</span> <span class="kn">import</span> <span class="n">default_hooks</span> <span class="k">as</span> <span class="n">dh</span>

<span class="n">zero_hook</span> <span class="o">=</span> <span class="nf">hook_with_zero_step</span><span class="p">(</span><span class="n">dh</span><span class="p">.</span><span class="n">allreduce_hook</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">register_comm_hook</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">hook</span><span class="o">=</span><span class="n">zero_hook</span><span class="p">)</span>

</code></pre></div></div> <p>After enabling overlap, the computation times are almost the same as without sharding.</p> <table> <thead> <tr> <th>Metric</th> <th>No sharding</th> <th>ZeRO - no overlap</th> <th>ZeRO + overlap</th> </tr> </thead> <tbody> <tr> <td>Memory.</td> <td>35.00 GB</td> <td>28.52 GB</td> <td>28.52 GB</td> </tr> <tr> <td>Time/step</td> <td>2251.12 ms</td> <td>2358.40 ms</td> <td>2275.07 ms</td> </tr> <tr> <td>MFU</td> <td>54.58%</td> <td>52.31%</td> <td>54.35%</td> </tr> </tbody> </table> <h3 id="saving-the-optimizer-state">Saving the optimizer state</h3> <p>Since the optimizer states are sharded, before saving the <code class="language-plaintext highlighter-rouge">state_dict</code>, we need to synchronize the states from all the devices. In PyTorch this can be done as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># called on all ranks, to=0 indicates that rank 0
# will have the consolidated optimizer states
</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">consolidate_state_dict</span><span class="p">(</span><span class="n">to</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span> <span class="sh">'</span><span class="s">optim_state.pth</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>This is a blocking step and it takes some time to sync among the devices. The time taken depends on the hardware setup, specifically the device interconnect.</p> <h3 id="summary">Summary</h3> <p>The optimizer states take up a significant amount of memory. However they can be sharded, and with the right implementation details, the communication cost involved is negligible. This enables training in some configurations which would either consume too much memory per device. As such, if an efficient implementation is possible, sharding should be enabled by default.</p> <p>However, the implementation of <a href="https://docs.pytorch.org/docs/main/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer" rel="external nofollow noopener" target="_blank">ZeRO stage-1</a> has some limitations. For example, we are not able to use fused AdamW and the code to add communication hooks is a bit cumbersome. The gradients are also not sharded. Fortunately, we have <a href="https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html" rel="external nofollow noopener" target="_blank">FSDP2</a> which provides ZeRO stage-1 and stage-2. We will explore this in the next post.</p> <h4 id="references">References:</h4> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p><a href="https://discuss.pytorch.org/t/how-to-use-torch-distributed-optim-zeroredundancyoptimizer-with-overlap-with-ddp-true/151523/3" rel="external nofollow noopener" target="_blank">https://discuss.pytorch.org/t/how-to-use-torch-distributed-optim-zeroredundancyoptimizer-with-overlap-with-ddp-true/151523/3</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Anuvabh Dutt. Built with <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> and <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://cdn.jsdelivr.net/npm/bootstrap-table@1.22.4/dist/bootstrap-table.min.js" integrity="sha256-4rppopQE9POKfukn2kEvhJ9Um25Cf6+IDVkARD0xh78=" crossorigin="anonymous"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>