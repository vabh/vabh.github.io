<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://vabh.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://vabh.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-26T17:01:34+00:00</updated><id>https://vabh.github.io/feed.xml</id><title type="html">blank</title><subtitle>Mostly notes for myself </subtitle><entry><title type="html">ZeRO-1</title><link href="https://vabh.github.io/blog/2025/zero/" rel="alternate" type="text/html" title="ZeRO-1"/><published>2025-10-12T14:00:00+00:00</published><updated>2025-10-12T14:00:00+00:00</updated><id>https://vabh.github.io/blog/2025/zero</id><content type="html" xml:base="https://vabh.github.io/blog/2025/zero/"><![CDATA[<h2 id="exploring-zero">Exploring ZeRO</h2> <p>This post explores the use of <a href="https://docs.pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer" target="_blank">ZeRO optimizer</a> from PyTorch. This is useful when training with DDP and you are bottlenecked by GPU memory. ZeRO optimizer shards the optimizer states across devices, leading to reduced memory usage per device. The <a href="https://arxiv.org/pdf/1910.02054">ZeRO paper</a> goes into details. Here we explore ZeRO stage-1.</p> <p>There is really no reason not to shard the optimizer states. It’s relatively straightforward to use with the ZeRO API. Note that there are some limitations, which will go away when we migrate to FSDP. However, the point of this post is to show that adding a few lines can already be beneficial.</p> <p>Before, going into the code, let’s review what it means to shard the “optimizer states” and how much memory we can expect to save. The small gpt config in <a href="https://github.com/karpathy/nanoGPT" target="_blank">nanoGPT</a> has \(124M\) params. If you load it up and do a training run, you will see that the memory usage is around \(12.3\) GiB.</p> <h2 id="memory-usage">Memory usage</h2> <p>Let’s break down this memory usage. A good reference is <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=memory_for_weights/grads/optimizer_states">here</a>. GPU memory comprises the following:</p> <ul> <li>M_p: The model parameters</li> <li>M_p32: A copy of the parameters in full precision, when using mixed precision training</li> <li>M_g: Gradients</li> <li>M_opt: Optimizer states (momentum in SGD, momentum and variance in Adam)</li> <li>M_act: Activations</li> <li>Other things like cuda kernels and temporary buffers</li> </ul> <p>Besides the activations, the memory for the \(124M\) model and get the memory used by activations (and the overhead things). This will let us estimate the memory usage after sharding the optimizer states.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Total parameters: N = 124_000_000
GPU memory per device: M = 12.3 * 2 ** 30 bytes

M_p = 2 * N # It takes two bytes to store 1 parameter in float16 / bfloat16
M_g = 2 * N;
M_p32 = 4 * N;
M_opt = 8 * N

The memory for activations is:
M_act = M - M_p - M_g - M_p32 - M_opt
M_act = 10.49 * 2 ** 30 bytes

If we have 8 GPUs, and we shard the optimizer across them, 
we expect the memory usage to be:
M_zero1 = M_act + M_p + M_g + M_p32 + M_opt / dp = 11.53
</code></pre></div></div> <p>The memory savings is entirely dependent on <code class="language-plaintext highlighter-rouge">dp</code>. For the \(124M\) model, around 6% memory savings are to be expected.</p> <p>PyTorch makes it really easy to instantiate the ZeRO version of an optimizer.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.distributed.optim</span> <span class="kn">import</span> <span class="n">ZeroRedundancyOptimizer</span> <span class="k">as</span> <span class="n">ZeRO</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="nc">ZeRO</span><span class="p">(</span>
    <span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span>
    <span class="n">optimizer_class</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">,</span>
    <span class="n">parameters_as_bucket_view</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
    <span class="n">overlap_with_ddp</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># we will get to this later
</span>    <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
    <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>Let’s now instantiate models of varying sizes. The sequence length is \(1024\) and the microbatch size is \(12\).</p> <table> <thead> <tr> <th>Memory</th> <th>124M</th> <th>353M</th> <th>772M</th> </tr> </thead> <tbody> <tr> <td>without sharding</td> <td>12.34 GB</td> <td>23.20 GB</td> <td><span style="color:#d9534f;">OOM</span></td> </tr> <tr> <td>with sharding</td> <td>11.15 GB</td> <td>21.26 GB</td> <td>35.93 GB</td> </tr> </tbody> </table> <p>We see that the actual memory usage is what our calculation above predicted. We also see that keeping the batch size fixed, the \(772M\) model is ‘out-of-memory’ without sharding.</p> <p>So is this a free lunch? Since we split the optimizer state across different devices, the devices need to communicate and update the copy of their weights accordingly. So whenever an optimization step happens, the weights need to be synced.</p> <h2 id="communication-cost">Communication cost</h2> <p>Let’s expand the table from above and look at the time it takes for each optimization step, on 8 * A100s.</p> <table> <thead> <tr> <th>Metric</th> <th>ZeRO</th> <th>124M</th> <th>353M</th> <th>772M</th> </tr> </thead> <tbody> <tr> <td>Memory</td> <td>✗</td> <td>12.34 GB</td> <td>23.20 GB</td> <td>OOM</td> </tr> <tr> <td>Memory</td> <td>✓</td> <td>11.15 GB</td> <td>21.26 GB</td> <td>35.93 GB</td> </tr> <tr> <td>Time/step</td> <td>✗</td> <td>345.83 ms</td> <td>907.96 ms</td> <td>—</td> </tr> <tr> <td>Time/step</td> <td>✓</td> <td>367.77 ms</td> <td>962.39 ms</td> <td>1943.65 ms</td> </tr> <tr> <td>MFU</td> <td>✗</td> <td>48.61%</td> <td>52.96%</td> <td>—</td> </tr> <tr> <td>MFU</td> <td>✓</td> <td>46.05%</td> <td>49.95%</td> <td>52.68%</td> </tr> </tbody> </table> <p>We see that the way we have used ZeRO, it is slightly slower than the non-sharded version. However, the good news is that we can overlap the communication step with the DDP communication. This needs a bit of extra work. You have to add specific hooks and not call the optimizer step directly. It will be done by the hook.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Source: https://discuss.pytorch.org/t/how-to-use-torch-distributed-optim-zeroredundancyoptimizer-with-overlap-with-ddp-true/151523/3
</span>
<span class="kn">from</span> <span class="n">torch.distributed.algorithms.ddp_comm_hooks.ddp_zero_hook</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">hook_with_zero_step</span><span class="p">,</span> 
    <span class="n">hook_with_zero_step_interleaved</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="n">torch.distributed.algorithms.ddp_comm_hooks</span> <span class="kn">import</span> <span class="n">default_hooks</span> <span class="k">as</span> <span class="n">dh</span>

<span class="n">zero_hook</span> <span class="o">=</span> <span class="nf">hook_with_zero_step</span><span class="p">(</span><span class="n">dh</span><span class="p">.</span><span class="n">allreduce_hook</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">register_comm_hook</span><span class="p">(</span><span class="n">state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">hook</span><span class="o">=</span><span class="n">zero_hook</span><span class="p">)</span>

</code></pre></div></div> <p>After enabling overlap, the computation times are almost the same as without sharding.</p> <table> <thead> <tr> <th>Metric</th> <th>No sharding</th> <th>ZeRO - no overlap</th> <th>ZeRO + overlap</th> </tr> </thead> <tbody> <tr> <td>Memory.</td> <td>35.00 GB</td> <td>28.52 GB</td> <td>28.52 GB</td> </tr> <tr> <td>Time/step</td> <td>2251.12 ms</td> <td>2358.40 ms</td> <td>2275.07 ms</td> </tr> <tr> <td>MFU</td> <td>54.58%</td> <td>52.31%</td> <td>54.35%</td> </tr> </tbody> </table> <h2 id="saving-the-optimizer-state">Saving the optimizer state</h2> <p>Since the optimizer states are sharded, before saving, we need to synchronize the states from all the devices. In PyTorch this can be done as:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># called on all ranks, to=0 indicates that rank 0
# will have the consolidated optimizer states
</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">consolidate_state_dict</span><span class="p">(</span><span class="n">to</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span><span class="p">.</span><span class="nf">get_rank</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
    <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>
</code></pre></div></div> <h2 id="summary">Summary</h2> <p>The optimizer states take up a significant amount of memory. However they can be shared and with the right implementation details, the communication cost involved is negligible. This enables training in some configurations which would not be possible. As such, if an efficient implementation is possible, sharding should be enabled by default.</p> <p>However, the implementation of <a href="https://docs.pytorch.org/docs/main/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer">ZeRO stage-1</a> has some limitations. For example, we are not able to use fused AdamW and the code to add communication hooks is a bit cumbersome. The gradients are also not sharded. Fortunately, we have <a href="https://docs.pytorch.org/docs/stable/distributed.fsdp.fully_shard.html">FSDP2</a> which provides ZeRO stage-1 and stage-2. We will explore this in the next post.</p>]]></content><author><name></name></author><category term="pytorch"/><category term="ml"/><category term="demo"/><summary type="html"><![CDATA[ZeRO stage 1 exploration using native PyTorch methods]]></summary></entry></feed>